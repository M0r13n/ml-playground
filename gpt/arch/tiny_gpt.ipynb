{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f37bebf-a316-4a3b-b80f-84b89f9c44ca",
   "metadata": {},
   "source": [
    "# An implementation of GPT2 in tinygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79319f7-7d66-411a-8a8b-55865e9ad1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a386776-d72a-4fd5-991b-84452b220884",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d48bbc4-3f0c-4f68-8430-9dbe98904f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "import tiktoken\n",
    "\n",
    "from tinygrad import Tensor, nn\n",
    "\n",
    "\n",
    "class TinyEncoder:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    def __call__(self, text:str) -> Tensor:\n",
    "        token_ids = self.tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        token_ids = Tensor(token_ids)\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, idx: Tensor) -> str:\n",
    "        return self.tokenizer.decode()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6c6a65-59cd-4439-af54-521e673c04ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20479"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as r\n",
    "\n",
    "\n",
    "response = r.get(\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "response.raise_for_status()\n",
    "content = response.text\n",
    "text = content\n",
    "pathlib.Path('the-verdict.txt').write_text(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950c09d8-8912-418c-b75f-45e8e3ce71a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  40  367 2885 ...  286 1242  526] (5145,)\n"
     ]
    }
   ],
   "source": [
    "Tensor.manual_seed(42)\n",
    "encoder = TinyEncoder()\n",
    "embedding = encoder(text)\n",
    "print(embedding.numpy(), embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed36a0-6e59-4a87-855d-b6f98f1951dd",
   "metadata": {},
   "source": [
    "### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2dacd64-85e7-4019-9740-c8cfa4172455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tinygrad import Tensor, nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention:\n",
    "\n",
    "    def __init__(self, d_in: int, d_out: int, context_length: int, n_heads: int, dropout: float = 0.0, bias=False) -> None:\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.context_length = context_length\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.d_out // n_heads\n",
    "\n",
    "        assert self.d_out % n_heads == 0\n",
    "\n",
    "        # Linear Projections\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=bias)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=bias)\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        # linear projections (B, num_tokens, d_out)\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        # split heads (B, num_tokens, d_out) -> (B, n_heads, num_tokens, head_dim)\n",
    "        keys = keys.reshape(b, num_tokens, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        queries = queries.reshape(b, num_tokens, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        values = values.reshape(b, num_tokens, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        attention_scores = queries @ keys.transpose(2,3)\n",
    "        causal_mask = Tensor.triu(Tensor.ones(self.context_length, self.context_length), diagonal=1).bool()\n",
    "        attention_scores = attention_scores.masked_fill(causal_mask[:num_tokens, :num_tokens], -math.inf)\n",
    "\n",
    "        attention_weights = (attention_scores / self.head_dim ** 0.5).softmax(axis=-1)\n",
    "        attention_weights = attention_weights.dropout(self.dropout)   \n",
    "\n",
    "        context_vec = attention_weights @ values\n",
    "\n",
    "        # (B, n_heads, num_tokens, head_dim) -> (B, num_tokens, d_out)\n",
    "        #context_vec = context_vec.view(b, num_tokens, self.d_out)\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4f8ac7-6e6a-47c4-8f0c-27b9b43920b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.27879322 0.14128757 0.8230009  ... 0.1827184  0.1913662  0.68339896]\n",
      "  [0.9142418  0.57308066 0.0665828  ... 0.5784267  0.518114   0.94284415]\n",
      "  [0.6814923  0.8197552  0.35364985 ... 0.8681469  0.6167911  0.40702832]\n",
      "  [0.9418659  0.19443953 0.53458285 ... 0.6712527  0.6055695  0.97542334]\n",
      "  [0.8606286  0.46927452 0.9223504  ... 0.21595609 0.36715853 0.6788553 ]]\n",
      "\n",
      " [[0.0319432  0.20642948 0.38278043 ... 0.5310552  0.7954571  0.79090667]\n",
      "  [0.32470655 0.442878   0.72512054 ... 0.68264794 0.9761251  0.9265356 ]\n",
      "  [0.6696094  0.87599456 0.764524   ... 0.2146635  0.25441122 0.75764453]\n",
      "  [0.31806588 0.41951966 0.43686438 ... 0.4907968  0.6730211  0.22261024]\n",
      "  [0.05004025 0.40175128 0.3989781  ... 0.5728892  0.9418839  0.5127858 ]]] (2, 5, 768)\n",
      "[[[ 0.15800107  0.02312575 -0.16923574 ... -0.13944025 -0.01949698\n",
      "   -0.01718881]\n",
      "  [ 0.20369866 -0.0435437  -0.1739159  ... -0.07403572 -0.0258761\n",
      "   -0.15652084]\n",
      "  [ 0.21391213 -0.0760002  -0.15109655 ... -0.07401244 -0.09526633\n",
      "   -0.11675066]\n",
      "  [ 0.22921634 -0.09602045 -0.1543265  ... -0.05955929 -0.07208632\n",
      "   -0.12802427]\n",
      "  [ 0.23849753 -0.14008355 -0.13567889 ... -0.03619739 -0.07867649\n",
      "   -0.13799292]]\n",
      "\n",
      " [[ 0.24363592  0.00585755 -0.14158955 ... -0.07455315 -0.34485978\n",
      "   -0.07858321]\n",
      "  [ 0.16663742  0.04177158 -0.13408996 ... -0.03623439 -0.21983841\n",
      "   -0.15593472]\n",
      "  [ 0.17236531  0.05069429 -0.12837097 ... -0.00328004 -0.21010816\n",
      "   -0.15915895]\n",
      "  [ 0.1713019   0.03347015 -0.12878507 ...  0.03676563 -0.18679643\n",
      "   -0.17976904]\n",
      "  [ 0.15524623  0.01007734 -0.12587646 ...  0.02108978 -0.18530926\n",
      "   -0.18487948]]] (2, 5, 768)\n"
     ]
    }
   ],
   "source": [
    "# Basic tests\n",
    "attention = MultiHeadAttention(\n",
    "    d_in=GPT_CONFIG_124M['emb_dim'],\n",
    "    d_out=GPT_CONFIG_124M['emb_dim'],\n",
    "    context_length=GPT_CONFIG_124M['context_length'],\n",
    "    n_heads=GPT_CONFIG_124M['n_heads'],\n",
    "    dropout=GPT_CONFIG_124M['drop_rate'],\n",
    "    bias=GPT_CONFIG_124M['qkv_bias']  \n",
    ")\n",
    "\n",
    "Tensor.manual_seed(42)\n",
    "\n",
    "x = Tensor.rand(2,5,768)\n",
    "y = attention(x)\n",
    "\n",
    "print(x.numpy(), x.shape)\n",
    "print(y.numpy(), y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2148195-aa7f-49d3-9752-2cba17fdb1d6",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee40db66-bdf2-44a0-8e3b-52ffb582975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tinygrad import Tensor\n",
    "\n",
    "\n",
    "class GELU:\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        result = x + 0.044715 * x.pow(3)\n",
    "        result = (2 / math.pi) ** 0.5 * result\n",
    "        result = 1 + result.tanh()\n",
    "        result = 0.5 * x * result\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ff3070d-bd55-409c-80a1-5dc07ed06ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu = GELU()\n",
    "\n",
    "def linspace(start, end, steps):\n",
    "    return Tensor.arange(steps) * ((end - start) / (steps - 1)) + start\n",
    "\n",
    "x = linspace(-3, 3, 100)\n",
    "y_gelu = gelu(x)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.title(f\"TinyGELU activation function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"TinyGELU(x)\")\n",
    "plt.grid(True)\n",
    "plt.plot(x.numpy(), y_gelu.numpy())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea8b376-ab4c-4613-9e13-e00f247a5496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "\n",
    "    def __init__(self, emb_dim: int):\n",
    "        self.emb_dim = emb_dim\n",
    "        self.expansion =  nn.Linear(emb_dim, emb_dim * 4, bias=True)    # Expansion\n",
    "        self.activation = GELU()                                         # Activation\n",
    "        self.projection = nn.Linear(4 * emb_dim, emb_dim, bias=True)    # Projection (reduction)\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        x = self.expansion(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5                                           # Epsilon\n",
    "        self.scale = Tensor.ones(emb_dim, requires_grad=True)     # Gamma\n",
    "        self.shift = Tensor.zeros(emb_dim, requires_grad=True)    # Beta\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        mean = x.mean(axis=-1, keepdim=True)\n",
    "        var = x.var(axis=-1, keepdim=True,  correction=False)\n",
    "        norm_x = (x - mean) / (var + self.eps).sqrt()\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class TransformerBlock:\n",
    "\n",
    "    def __init__(self, cfg: dict[str, object]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_in=cfg['emb_dim'],\n",
    "            d_out=cfg['emb_dim'],\n",
    "            context_length=cfg['context_length'],\n",
    "            n_heads=cfg['n_heads'],\n",
    "            dropout=cfg['drop_rate'],\n",
    "            bias=cfg['qkv_bias']  \n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(cfg['emb_dim'])\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_rate = cfg['drop_rate']  # applied at multiple levels to prevent overfitting at each level\n",
    "\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = x.dropout(self.drop_rate)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x.dropout(self.drop_rate)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc73b3ed-5c2d-4674-8739-e753aaccb2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.38344753 0.44372582 0.53390443 ... 0.9143466  0.74681854 0.52751124]\n",
      "  [0.0514853  0.5059414  0.62324333 ... 0.32642472 0.74234366 0.33380938]\n",
      "  [0.39911366 0.9910134  0.8848647  ... 0.7345259  0.94840825 0.22873878]\n",
      "  [0.81257474 0.00250328 0.02336907 ... 0.02043915 0.7478622  0.9520525 ]\n",
      "  [0.02613831 0.32583916 0.29159844 ... 0.7147529  0.43251014 0.7831538 ]]\n",
      "\n",
      " [[0.0979892  0.43188393 0.9960046  ... 0.72611034 0.53721416 0.62705123]\n",
      "  [0.6718372  0.252977   0.24173808 ... 0.05607057 0.60751295 0.41790247]\n",
      "  [0.8334737  0.5323237  0.7109926  ... 0.5211289  0.5110619  0.17008722]\n",
      "  [0.6909704  0.68336666 0.9174044  ... 0.37833953 0.5252744  0.5231186 ]\n",
      "  [0.37710595 0.6624342  0.2080679  ... 0.32915902 0.1228075  0.13681471]]]\n",
      "[[[ 0.47687268  0.13560627  0.69710654 ...  0.92800057  0.6507147\n",
      "    1.3987957 ]\n",
      "  [ 0.08492064  0.74069506  0.82944876 ...  0.69849324  1.0212165\n",
      "    0.66632164]\n",
      "  [ 0.64549166  0.99568987  0.7445702  ...  0.6836327   1.0542799\n",
      "    0.31632346]\n",
      "  [ 0.8125977   0.44991156 -0.19266991 ...  0.58830535  0.71917725\n",
      "    0.944429  ]\n",
      "  [ 0.02137763  0.3029822   0.13000886 ...  0.78760546  0.48615712\n",
      "    0.9012226 ]]\n",
      "\n",
      " [[ 0.08515473  1.036847    1.1054221  ...  0.8012278   0.19930962\n",
      "    0.7074624 ]\n",
      "  [ 1.1535919   0.03052857  0.11081418 ...  0.21723998  0.49545884\n",
      "    0.8694265 ]\n",
      "  [ 1.2701054   0.44222063  0.70227104 ...  0.6521343   0.7089205\n",
      "    0.3142791 ]\n",
      "  [ 0.9159672   0.97362745  1.4827415  ...  0.34846562  0.45540804\n",
      "    0.636935  ]\n",
      "  [ 0.43280822  0.48143005  0.5184246  ...  0.4269638   0.09086804\n",
      "    0.1251043 ]]]\n"
     ]
    }
   ],
   "source": [
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "x = Tensor.rand(2,5,768)\n",
    "y = block(x)\n",
    "\n",
    "print(x.numpy())\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6be94-16a6-4cea-ae1f-796120f16d00",
   "metadata": {},
   "source": [
    "### GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ecd237a-3566-44ac-ab5c-9b81762586cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                           GPT-2 ARCHITECTURE                                 ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "┌─────────────────────────────────────────────────────────────────────────────┐\n",
      "│  INPUT                                                                      │\n",
      "│  \"The cat sat on the\"                                                       │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "                                      │\n",
      "                                      ▼\n",
      "┌─────────────────────────────────────────────────────────────────────────────┐\n",
      "│  TOKENIZER (Byte-Pair Encoding)                                             │\n",
      "│  \"The cat sat on the\" → [464, 3797, 3332, 319, 262]                         │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "                                      │\n",
      "                                      ▼\n",
      "┌─────────────────────────────────────────────────────────────────────────────┐\n",
      "│  TOKEN EMBEDDING                          POSITIONAL EMBEDDING              │\n",
      "│  vocab_size (50257) × d_model (768)       max_seq_len (1024) × d_model      │\n",
      "│           │                                         │                       │\n",
      "│           └──────────────┬──────────────────────────┘                       │\n",
      "│                          ▼                                                  │\n",
      "│                    Element-wise ADD                                         │\n",
      "│                          │                                                  │\n",
      "│                          ▼                                                  │\n",
      "│              Combined Embeddings [seq_len, 768]                             │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "                                      │\n",
      "                                      ▼\n",
      "┌─────────────────────────────────────────────────────────────────────────────┐\n",
      "│                                                                             │\n",
      "│   ╔═══════════════════════════════════════════════════════════════════╗     │\n",
      "│   ║  TRANSFORMER BLOCK (×12 for GPT-2 Small)                          ║     │\n",
      "│   ╠═══════════════════════════════════════════════════════════════════╣     │\n",
      "│   ║                                                                   ║     │\n",
      "│   ║   Input ──┬──────────────────────────────────────┐                ║     │\n",
      "│   ║           │                                      │                ║     │\n",
      "│   ║           ▼                                      │                ║     │\n",
      "│   ║   ┌───────────────┐                              │                ║     │\n",
      "│   ║   │  Layer Norm 1 │                              │                ║     │\n",
      "│   ║   └───────┬───────┘                              │                ║     │\n",
      "│   ║           │                                      │                ║     │\n",
      "│   ║           ▼                                      │                ║     │\n",
      "│   ║   ┌───────────────────────────────────────┐      │                ║     │\n",
      "│   ║   │  Masked Multi-Head Self-Attention     │      │                ║     │\n",
      "│   ║   │  (12 heads, d_k = 64 per head)        │      │                ║     │\n",
      "│   ║   │                                       │      │                ║     │\n",
      "│   ║   │   Q ─┐                                │      │                ║     │\n",
      "│   ║   │   K ─┼─► Attention ─► Concat ─► Proj  │      │                ║     │\n",
      "│   ║   │   V ─┘                                │      │                ║     │\n",
      "│   ║   └───────────────┬───────────────────────┘      │                ║     │\n",
      "│   ║                   │                              │                ║     │\n",
      "│   ║                   ▼                              │                ║     │\n",
      "│   ║               (+) ADD ◄──────────────────────────┘  Residual 1    ║     │\n",
      "│   ║                   │                                               ║     │\n",
      "│   ║           ┌───────┴──────────────────────────────┐                ║     │\n",
      "│   ║           │                                      │                ║     │\n",
      "│   ║           ▼                                      │                ║     │\n",
      "│   ║   ┌───────────────┐                              │                ║     │\n",
      "│   ║   │  Layer Norm 2 │                              │                ║     │\n",
      "│   ║   └───────┬───────┘                              │                ║     │\n",
      "│   ║           │                                      │                ║     │\n",
      "│   ║           ▼                                      │                ║     │\n",
      "│   ║   ┌───────────────────────────────────────┐      │                ║     │\n",
      "│   ║   │  Feed-Forward Network                 │      │                ║     │\n",
      "│   ║   │                                       │      │                ║     │\n",
      "│   ║   │  Linear(768 → 3072)                   │      │                ║     │\n",
      "│   ║   │       │                               │      │                ║     │\n",
      "│   ║   │       ▼                               │      │                ║     │\n",
      "│   ║   │     GELU                              │      │                ║     │\n",
      "│   ║   │       │                               │      │                ║     │\n",
      "│   ║   │       ▼                               │      │                ║     │\n",
      "│   ║   │  Linear(3072 → 768)                   │      │                ║     │\n",
      "│   ║   └───────────────┬───────────────────────┘      │                ║     │\n",
      "│   ║                   │                              │                ║     │\n",
      "│   ║                   ▼                              │                ║     │\n",
      "│   ║               (+) ADD ◄──────────────────────────┘  Residual 2    ║     │\n",
      "│   ║                   │                                               ║     │\n",
      "│   ║                   ▼                                               ║     │\n",
      "│   ║               Output                                              ║     │\n",
      "│   ║                                                                   ║     │\n",
      "│   ╚═══════════════════════════════════════════════════════════════════╝     │\n",
      "│                                      │                                      │\n",
      "│                              (Repeat ×12)                                   │\n",
      "│                                                                             │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "                                      │\n",
      "                                      ▼\n",
      "┌─────────────────────────────────────────────────────────────────────────────┐\n",
      "│  FINAL LAYER NORM                                                           │\n",
      "│  LayerNorm(768)                                                             │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "                                      │\n",
      "                                      ▼\n",
      "┌─────────────────────────────────────────────────────────────────────────────┐\n",
      "│  OUTPUT PROJECTION (Language Model Head)                                    │\n",
      "│  Linear(768 → 50257)  [Weight tied with token embeddings]                   │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "                                      │\n",
      "                                      ▼\n",
      "┌─────────────────────────────────────────────────────────────────────────────┐\n",
      "│  SOFTMAX                                                                    │\n",
      "│  Probability distribution over vocabulary                                   │\n",
      "│  P(\"mat\" | \"The cat sat on the\") = 0.12                                     │\n",
      "│  P(\"floor\" | \"The cat sat on the\") = 0.08                                   │\n",
      "│  ...                                                                        │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "                                      │\n",
      "                                      ▼\n",
      "┌─────────────────────────────────────────────────────────────────────────────┐\n",
      "│  OUTPUT                                                                     │\n",
      "│  Next token prediction: \"mat\" (or sample from distribution)                 │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 Architecture Overview - Jupyter Visualization\n",
    "\n",
    "gpt2_architecture = \"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                           GPT-2 ARCHITECTURE                                 ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  INPUT                                                                      │\n",
    "│  \"The cat sat on the\"                                                       │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  TOKENIZER (Byte-Pair Encoding)                                             │\n",
    "│  \"The cat sat on the\" → [464, 3797, 3332, 319, 262]                         │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  TOKEN EMBEDDING                          POSITIONAL EMBEDDING              │\n",
    "│  vocab_size (50257) × d_model (768)       max_seq_len (1024) × d_model      │\n",
    "│           │                                         │                       │\n",
    "│           └──────────────┬──────────────────────────┘                       │\n",
    "│                          ▼                                                  │\n",
    "│                    Element-wise ADD                                         │\n",
    "│                          │                                                  │\n",
    "│                          ▼                                                  │\n",
    "│              Combined Embeddings [seq_len, 768]                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                                                                             │\n",
    "│   ╔═══════════════════════════════════════════════════════════════════╗     │\n",
    "│   ║  TRANSFORMER BLOCK (×12 for GPT-2 Small)                          ║     │\n",
    "│   ╠═══════════════════════════════════════════════════════════════════╣     │\n",
    "│   ║                                                                   ║     │\n",
    "│   ║   Input ──┬──────────────────────────────────────┐                ║     │\n",
    "│   ║           │                                      │                ║     │\n",
    "│   ║           ▼                                      │                ║     │\n",
    "│   ║   ┌───────────────┐                              │                ║     │\n",
    "│   ║   │  Layer Norm 1 │                              │                ║     │\n",
    "│   ║   └───────┬───────┘                              │                ║     │\n",
    "│   ║           │                                      │                ║     │\n",
    "│   ║           ▼                                      │                ║     │\n",
    "│   ║   ┌───────────────────────────────────────┐      │                ║     │\n",
    "│   ║   │  Masked Multi-Head Self-Attention     │      │                ║     │\n",
    "│   ║   │  (12 heads, d_k = 64 per head)        │      │                ║     │\n",
    "│   ║   │                                       │      │                ║     │\n",
    "│   ║   │   Q ─┐                                │      │                ║     │\n",
    "│   ║   │   K ─┼─► Attention ─► Concat ─► Proj  │      │                ║     │\n",
    "│   ║   │   V ─┘                                │      │                ║     │\n",
    "│   ║   └───────────────┬───────────────────────┘      │                ║     │\n",
    "│   ║                   │                              │                ║     │\n",
    "│   ║                   ▼                              │                ║     │\n",
    "│   ║               (+) ADD ◄──────────────────────────┘  Residual 1    ║     │\n",
    "│   ║                   │                                               ║     │\n",
    "│   ║           ┌───────┴──────────────────────────────┐                ║     │\n",
    "│   ║           │                                      │                ║     │\n",
    "│   ║           ▼                                      │                ║     │\n",
    "│   ║   ┌───────────────┐                              │                ║     │\n",
    "│   ║   │  Layer Norm 2 │                              │                ║     │\n",
    "│   ║   └───────┬───────┘                              │                ║     │\n",
    "│   ║           │                                      │                ║     │\n",
    "│   ║           ▼                                      │                ║     │\n",
    "│   ║   ┌───────────────────────────────────────┐      │                ║     │\n",
    "│   ║   │  Feed-Forward Network                 │      │                ║     │\n",
    "│   ║   │                                       │      │                ║     │\n",
    "│   ║   │  Linear(768 → 3072)                   │      │                ║     │\n",
    "│   ║   │       │                               │      │                ║     │\n",
    "│   ║   │       ▼                               │      │                ║     │\n",
    "│   ║   │     GELU                              │      │                ║     │\n",
    "│   ║   │       │                               │      │                ║     │\n",
    "│   ║   │       ▼                               │      │                ║     │\n",
    "│   ║   │  Linear(3072 → 768)                   │      │                ║     │\n",
    "│   ║   └───────────────┬───────────────────────┘      │                ║     │\n",
    "│   ║                   │                              │                ║     │\n",
    "│   ║                   ▼                              │                ║     │\n",
    "│   ║               (+) ADD ◄──────────────────────────┘  Residual 2    ║     │\n",
    "│   ║                   │                                               ║     │\n",
    "│   ║                   ▼                                               ║     │\n",
    "│   ║               Output                                              ║     │\n",
    "│   ║                                                                   ║     │\n",
    "│   ╚═══════════════════════════════════════════════════════════════════╝     │\n",
    "│                                      │                                      │\n",
    "│                              (Repeat ×12)                                   │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  FINAL LAYER NORM                                                           │\n",
    "│  LayerNorm(768)                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  OUTPUT PROJECTION (Language Model Head)                                    │\n",
    "│  Linear(768 → 50257)  [Weight tied with token embeddings]                   │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  SOFTMAX                                                                    │\n",
    "│  Probability distribution over vocabulary                                   │\n",
    "│  P(\"mat\" | \"The cat sat on the\") = 0.12                                     │\n",
    "│  P(\"floor\" | \"The cat sat on the\") = 0.08                                   │\n",
    "│  ...                                                                        │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "                                      │\n",
    "                                      ▼\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  OUTPUT                                                                     │\n",
    "│  Next token prediction: \"mat\" (or sample from distribution)                 │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\"\n",
    "\n",
    "print(gpt2_architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "778920c4-d852-443f-a468-d1211392a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.37475854  0.91921234  0.377479   ...  0.19478574  0.04634326\n",
      "   -0.5028892 ]\n",
      "  [-0.11838737  0.26292318  0.5065671  ... -0.10617074  0.45981625\n",
      "   -0.9972051 ]\n",
      "  [-0.09512917  0.26536468  0.66464454 ... -0.1491879   0.2909158\n",
      "   -0.98623925]\n",
      "  [-0.02449884  0.10951907  0.6436204  ... -0.24726972  0.35728818\n",
      "   -0.81317544]\n",
      "  [ 0.06015152  0.05567765  0.7080676  ... -0.3271778   0.30426148\n",
      "   -0.94280696]\n",
      "  [-0.03009875  0.07611722  0.83187443 ... -0.3352958   0.45460936\n",
      "   -0.6862832 ]]] (1, 6, 50257)\n"
     ]
    }
   ],
   "source": [
    "import tinygrad\n",
    "import tinygrad.nn\n",
    "\n",
    "class TinyGPTModel:\n",
    "    def __init__(self, cfg: dict[str, object]):\n",
    "        # Embedding\n",
    "        self.emb_layer = tinygrad.nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_layer = tinygrad.nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_rate = cfg[\"drop_rate\"]\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.trf_blocks = [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        \n",
    "        # Layer norms\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = tinygrad.nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=True)\n",
    "\n",
    "\n",
    "    def __call__(self, in_idx: tinygrad.Tensor) -> tinygrad.tensor:\n",
    "        num_batches, seq_len = in_idx.shape\n",
    "        embedding = self.emb_layer(in_idx)\n",
    "        pos_embeddings = self.pos_layer(tinygrad.Tensor.arange(seq_len))\n",
    "\n",
    "        x = embedding + pos_embeddings\n",
    "        x = x.dropout(self.drop_rate)\n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "tinygrad.Tensor.manual_seed(42)\n",
    "tiny = TinyGPTModel(GPT_CONFIG_124M)\n",
    "input_batch = encoder(\"Hello, how are you?\").unsqueeze(0)\n",
    "logits = tiny(input_batch)\n",
    "\n",
    "print(logits.numpy(), logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f04cf591-1b2c-4e60-9f88-101068a6277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 163,138,722\n",
      "Total parameters considering weight tying: 124,541,346\n",
      "Each feed forward module has 4,722,432 parameters, which is in sum 56,669,184.\n",
      "Each attention module has 2,362,368 parameters, which is in sum 28,348,416.\n"
     ]
    }
   ],
   "source": [
    "# count parameters\n",
    "\n",
    "from tinygrad.nn.state import get_parameters\n",
    "\n",
    "total_params = sum(p.numel() for p in get_parameters(tiny))\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "total_params_gpt2 = total_params - get_parameters(tiny.out_head)[0].numel()\n",
    "print(f\"Total parameters considering weight tying: {total_params_gpt2:,}\")  # Should be ~124M\n",
    "\n",
    "# Exercise 4.1\n",
    "# Calculate the number of params of the feed forward module and the multi head attention module\n",
    "ff_params = 0\n",
    "for p in get_parameters(tiny.trf_blocks[0].ff):\n",
    "    ff_params += p.numel()\n",
    "print(f'Each feed forward module has {ff_params:,} parameters, which is in sum {12*ff_params:,}.')\n",
    "\n",
    "attn_params = 0\n",
    "for p in get_parameters(tiny.trf_blocks[0].attention):\n",
    "    attn_params += p.numel()\n",
    "print(f'Each attention module has {attn_params:,} parameters, which is in sum {12*attn_params:,}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23a59e52-b296-4f02-91f4-ea2f4f2c9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinygrad.Tensor.manual_seed(42)\n",
    "tiny = TinyGPTModel(GPT_CONFIG_124M)\n",
    "input_batch = encoder(\"Hello, how are\").unsqueeze(0)\n",
    "logits = tiny(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e2c477-c4f0-4c09-8d1a-9df1271a2422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Input: [[15496    11   703   389]]\n",
      "Predicting next tokens - one at a time:\n",
      "[[15496    11   703   389  5071]]\n",
      "[[15496    11   703   389  5071  5071]]\n",
      "[[15496    11   703   389  5071  5071  5071]]\n",
      "[[15496    11   703   389  5071  5071  5071  5071]]\n",
      "[[15496    11   703   389  5071  5071  5071  5071  5071]]\n",
      "[[15496    11   703   389  5071  5071  5071  5071  5071  7735]]\n",
      "\n",
      "Generated text: Hello, how are discovered discovered discovered discovered discovered Further\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 6\n",
    "idx = input_batch\n",
    "context_size = 1024\n",
    "\n",
    "print(f'Encoded Input: {idx.numpy()}')\n",
    "print('Predicting next tokens - one at a time:')\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    # Limit the current context\n",
    "    idx_cond = idx[:, -context_size:]\n",
    "\n",
    "    # Calculate predictions from the last token.\n",
    "    # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "    logits = tiny(idx_cond)    \n",
    "    logits = logits[:, -1, :]\n",
    "\n",
    "    # Chose the highest probability value\n",
    "    probas = logits.softmax(axis=-1)\n",
    "    nxt = probas.argmax(axis=-1, keepdim=True)\n",
    "\n",
    "    # Append the predicted token to the current sequence\n",
    "    idx = idx.cat(nxt, dim=1)\n",
    "    print(idx.numpy())\n",
    "\n",
    "print('\\nGenerated text:', encoder.tokenizer.decode(idx.squeeze(0).tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef6bf1f8-cf3b-4faa-91f7-17ad4af3e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GPT2 weights\n",
    "import pathlib\n",
    "import requests as r\n",
    "\n",
    "BASE_URL = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "FILES = [\n",
    "    \"checkpoint\", \"encoder.json\", \"hparams.json\", \n",
    "    \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\", \n",
    "    \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "]\n",
    "MODEL_SIZE = \"124M\"\n",
    "MODELS_DIR = \"gpt2_weights\"\n",
    "\n",
    "models_dir = pathlib.Path(MODELS_DIR)\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for filename in FILES:\n",
    "        url = f\"{BASE_URL}/{MODEL_SIZE}/{filename}\"\n",
    "        filepath = models_dir.joinpath(filename)\n",
    "        if not filepath.exists():\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            response = r.get(url)\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfd8fced-2c0f-467e-8301-a55cdc2f75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensorflow checkpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_ckpt_path = models_dir.joinpath(\"model.ckpt\")\n",
    "params = {}\n",
    "\n",
    "for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
    "    arr = tf.train.load_variable(tf_ckpt_path, name)\n",
    "    params[name] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11cfe94b-b8fd-4a47-8d99-420ce7eaa10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_tensor(arr: np.array) -> Tensor:\n",
    "    return Tensor(arr.astype(np.float32))\n",
    "\n",
    "def assign(left: Tensor, right: Tensor):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f'Dimensions mismatch: {left.shape} != {right.shape}')\n",
    "    return right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2278f6-fcd6-4c1a-be90-da56ac7577b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_model = TinyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "tiny_model.emb_layer.weight = assign(tiny_model.emb_layer.weight, to_tensor(params[\"model/wte\"]))\n",
    "tiny_model.pos_layer.weight = assign(tiny_model.pos_layer.weight, to_tensor(params[\"model/wpe\"]))\n",
    "\n",
    "for i, block in enumerate(tiny_model.trf_blocks):\n",
    "    # Layer norms\n",
    "    block.norm1.scale = assign(block.norm1.scale, to_tensor(params[f\"model/h{i}/ln_1/g\"]))\n",
    "    block.norm1.shift = assign(block.norm1.shift, to_tensor(params[f\"model/h{i}/ln_1/b\"]))\n",
    "    block.norm2.scale = assign(block.norm2.scale, to_tensor(params[f\"model/h{i}/ln_2/g\"]))\n",
    "    block.norm2.shift = assign(block.norm2.shift, to_tensor(params[f\"model/h{i}/ln_2/b\"]))\n",
    "\n",
    "    # Attention - weights\n",
    "    qkv_w = params[f\"model/h{i}/attn/c_attn/w\"][0]  # Shape: (768, 2304)\n",
    "    q_w, k_w, v_w = np.split(qkv_w, 3, axis=-1)\n",
    "\n",
    "    block.attention.W_q.weight = assign(block.attention.W_q.weight, to_tensor(q_w.T))\n",
    "    block.attention.W_k.weight = assign(block.attention.W_k.weight, to_tensor(k_w.T))\n",
    "    block.attention.W_v.weight = assign(block.attention.W_v.weight, to_tensor(v_w.T))\n",
    "\n",
    "    # Attention - bias\n",
    "    qkv_b = params[f\"model/h{i}/attn/c_attn/b\"]\n",
    "    q_b, k_b, v_b = np.split(qkv_b, 3, axis=-1)\n",
    "\n",
    "    block.attention.W_q.bias = assign(block.attention.W_q.bias, to_tensor(q_b))\n",
    "    block.attention.W_k.bias = assign(block.attention.W_k.bias, to_tensor(k_b))\n",
    "    block.attention.W_v.bias = assign(block.attention.W_v.bias, to_tensor(v_b))\n",
    "\n",
    "    # Attention = Output projection\n",
    "    block.attention.out_proj.weight = assign(block.attention.out_proj.weight, to_tensor(params[f\"model/h{i}/attn/c_proj/w\"][0].T))\n",
    "    block.attention.out_proj.bias = assign(block.attention.out_proj.bias, to_tensor(params[f\"model/h{i}/attn/c_proj/b\"]))\n",
    "\n",
    "    # FFN\n",
    "    block.ff.expansion.weight = assign(block.ff.expansion.weight, to_tensor(params[f\"model/h{i}/mlp/c_fc/w\"][0].T))\n",
    "    block.ff.expansion.bias = assign(block.ff.expansion.bias, to_tensor(params[f\"model/h{i}/mlp/c_fc/b\"]))\n",
    "    block.ff.projection.weight = assign(block.ff.projection.weight, to_tensor(params[f\"model/h{i}/mlp/c_proj/w\"][0].T))\n",
    "    block.ff.projection.bias = assign(block.ff.projection.bias, to_tensor(params[f\"model/h{i}/mlp/c_proj/b\"]))\n",
    "    \n",
    "\n",
    "# Final norm\n",
    "tiny_model.final_norm.scale = to_tensor(params[\"model/ln_f/g\"])\n",
    "tiny_model.final_norm.shift = to_tensor(params[\"model/ln_f/b\"])\n",
    "\n",
    "# Output head\n",
    "tiny_model.out_head.weight = to_tensor(params[\"model/wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3f23d-39fd-4558-b41a-c438655a1798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world in Python, you can use the following to create a new Python script:\n",
      "\n",
      "from flask import Flask from flask.py import Flask_"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "input_batch = encoder(\"Hello world in Python\").unsqueeze(0)\n",
    "logits = tiny_model(input_batch)\n",
    "\n",
    "max_new_tokens = 100\n",
    "idx = input_batch\n",
    "context_size = 1024\n",
    "\n",
    "print(f'Encoded Input: {idx.numpy()}')\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    # Limit the current context\n",
    "    idx_cond = idx[:, -context_size:]\n",
    "\n",
    "    # Calculate predictions from the last token.\n",
    "    # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "    logits = tiny_model(idx_cond)    \n",
    "    logits = logits[:, -1, :]\n",
    "\n",
    "    # Chose the highest probability value\n",
    "    probas = logits.softmax(axis=-1)\n",
    "    nxt = probas.argmax(axis=-1, keepdim=True)\n",
    "\n",
    "    # Append the predicted token to the current sequence\n",
    "    idx = idx.cat(nxt, dim=1)\n",
    "    #print(idx.numpy())\n",
    "\n",
    "    output = encoder.tokenizer.decode(idx.squeeze(0).tolist())\n",
    "    clear_output(wait=True)\n",
    "    print(output, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d0f97-e50c-4502-9d83-04d5045eb5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
