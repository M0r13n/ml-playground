{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93812418-3a43-4efa-8f2a-ac4b69098bde",
   "metadata": {},
   "source": [
    "# GPT2 dummy architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7dd7e9-2e91-4469-b560-84b25d51bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61a7ae0b-b28a-4af1-a753-e6a8aa8c0f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "lines = [\n",
    "    \"Every effort moves you\",\n",
    "    \"Every day holds a\"    \n",
    "]\n",
    "\n",
    "batch = torch.stack([torch.tensor(tokenizer.encode(line)) for line in lines])\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85abd374-bea4-4b05-a56d-742fd340b2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg: dict[str, object]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding\n",
    "        self.emb_layer = torch.nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_layer = torch.nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Layer norms\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, in_idx: torch.Tensor) -> torch.tensor:\n",
    "        num_batches, seq_len = in_idx.shape\n",
    "        embedding = self.emb_layer(in_idx)\n",
    "        pos_embeddings = self.pos_layer(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        x = embedding + pos_embeddings\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "gpt_dummy = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = gpt_dummy(batch)\n",
    "\n",
    "print(logits, logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49387954-71e2-48d3-b000-3de4038e7503",
   "metadata": {},
   "source": [
    "Kurzformel für LayerNorm (pro Sample über die Feature-Dimension)\n",
    "\n",
    "Gegeben ein Vektor $x \\in \\mathbb{R}^H$:\n",
    "- Mittelwert: $\\mu = \\frac{1}{H}\\sum_{i=1}^{H} x_i$\n",
    "- Varianz: $\\sigma^2 = \\frac{1}{H}\\sum_{i=1}^{H} (x_i - \\mu)^2$\n",
    "- Normalisierung: $\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}$\n",
    "- Optionale affine Transformation: $y_i = \\gamma_i \\hat{x}_i + \\beta_i$\n",
    "\n",
    "Hinweis:\n",
    "- Bei Tensoren mit Shape [Batch, …, Features] werden $\\mu$ und $\\sigma^2$ für jede Zeile/Position über die letzte Dimension (Features) berechnet.\n",
    "- $\\varepsilon$ ist eine kleine Konstante für Numerikstabilität; $\\gamma,\\beta \\in \\mathbb{R}^H$ sind lernbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbfee818-542d-4683-aead-23b280b5b8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0455,  0.9326, -1.0036, -1.2364,  1.2619],\n",
      "        [ 0.8868,  0.9411, -1.8051, -0.1976,  0.1748]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.0000e+00],\n",
      "        [-2.3842e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(2, 5)\n",
    "mu = t.mean(dim=-1, keepdim=True)\n",
    "var = t.var(dim=-1, keepdim=True, unbiased=False)\n",
    "t_hat = (t - mu) / torch.sqrt(var + 1e-5)\n",
    "gamma = nn.Parameter(torch.ones(5))\n",
    "beta  = nn.Parameter(torch.zeros(5))\n",
    "out = gamma * t_hat + beta\n",
    "\n",
    "print(out)\n",
    "print(out.mean(dim=-1, keepdim=True))\n",
    "print(out.var(dim=-1, unbiased=False, keepdim=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce93491b-b02f-47ce-9de6-aa68059b757f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0455,  0.9326, -1.0036, -1.2364,  1.2619],\n",
      "        [ 0.8868,  0.9411, -1.8051, -0.1976,  0.1748]], grad_fn=<AddBackward0>)\n",
      "Mean:\n",
      " tensor([[ 0.0000e+00],\n",
      "        [-2.3842e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5                                   # Epsilon\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))    # Gamma\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))   # Beta\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(t)\n",
    "\n",
    "print(out_ln)\n",
    "print(\"Mean:\\n\",  out_ln.mean(dim=-1, keepdim=True))\n",
    "print(\"Variance:\\n\", out_ln.var(dim=-1, unbiased=False, keepdim=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee1c93d5-052c-49a2-b9a0-0e1c0dde0c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.519526    0.7537327   0.2005472  ...  1.0668471  -1.5136696\n",
      "   -0.33381417]\n",
      "  [-0.34302607 -0.42789376 -0.25507692 ... -0.4442997  -0.13493417\n",
      "    0.87539715]\n",
      "  [-0.13321133  0.28134     0.40881827 ... -0.44780132 -0.11065756\n",
      "    0.1352929 ]\n",
      "  [ 0.10996917  1.0887383   0.5327068  ... -0.424065   -0.12587118\n",
      "   -0.37713853]]\n",
      "\n",
      " [[-0.519526    0.7537327   0.2005472  ...  1.0668471  -1.5136696\n",
      "   -0.33381417]\n",
      "  [-0.02714658 -0.44134513 -0.16352235 ... -0.32524362 -0.22037949\n",
      "    0.8112958 ]\n",
      "  [-0.24537109  0.26612917  0.45883688 ... -0.4342     -0.00584923\n",
      "   -0.17356916]\n",
      "  [ 0.04278945  0.97872955  0.4680154  ... -0.31337032 -0.01501034\n",
      "   -0.2654793 ]]] (2, 4, 50257)\n"
     ]
    }
   ],
   "source": [
    "import tinygrad\n",
    "import tinygrad.nn\n",
    "\n",
    "\n",
    "class TinyLayerNorm:\n",
    "    def __init__(self, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5                                                    # Epsilon\n",
    "        self.scale = tinygrad.Tensor.ones(emb_dim, requires_grad=True)     # Gamma\n",
    "        self.shift = tinygrad.Tensor.zeros(emb_dim, requires_grad=True)    # Beta\n",
    "\n",
    "    def forward(self, x:tinygrad.Tensor) -> tinygrad.Tensor:\n",
    "        mean = x.mean(axis=-1, keepdim=True)\n",
    "        var = x.var(axis=-1, keepdim=True,  correction=False)\n",
    "        norm_x = (x - mean) / (var + self.eps).sqrt()\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class TinyGPTModel:\n",
    "    def __init__(self, cfg: dict[str, object]):\n",
    "        # Embedding\n",
    "        self.emb_layer = tinygrad.nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_layer = tinygrad.nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_rate = cfg[\"drop_rate\"]\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.trf_blocks = [DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        \n",
    "        # Layer norms\n",
    "        self.final_norm = TinyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = tinygrad.nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, in_idx: tinygrad.Tensor) -> tinygrad.tensor:\n",
    "        num_batches, seq_len = in_idx.shape\n",
    "        embedding = self.emb_layer(in_idx)\n",
    "        pos_embeddings = self.pos_layer(tinygrad.Tensor.arange(seq_len))\n",
    "\n",
    "        x = embedding + pos_embeddings\n",
    "        x = x.dropout(self.drop_rate)\n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x)\n",
    "        x = self.final_norm.forward(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "tiny_batch = tinygrad.Tensor.stack([tinygrad.Tensor(tokenizer.encode(line)) for line in lines])\n",
    "\n",
    "tinygrad.Tensor.manual_seed(42)\n",
    "tiny = TinyGPTModel(GPT_CONFIG_124M)\n",
    "logits = tiny.forward(tiny_batch)\n",
    "\n",
    "print(logits.numpy(), logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3a50d-872a-41f3-a526-115d9a72cce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affed578-6cfb-4dee-b7ba-93efbae3f232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ae09c-abde-425d-9b7f-56d3ec5f0d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38544f9-d9bc-4b63-817c-70b441a3297c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
