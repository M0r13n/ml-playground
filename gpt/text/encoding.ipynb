{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476463cc-d702-439d-aec0-2ded5dfe2133",
   "metadata": {},
   "source": [
    "# Part of https://github.com/rasbt/LLMs-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea14e673-8c6c-4c26-a762-32762004c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c07b154-8eba-40f0-baa4-0f5310b8122b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 34194, 78, 11, 10564, 318, 83, 304, 259, 8255, 410, 49252, 284, 6051, 370, 30570, 353, 198, 198, 23041, 46795, 83, 257, 794, 311, 8623, 2736, 41437, 532, 266, 494, 2208, 0, 198, 198, 3629, 72, 1086, 64, 1086, 549, 19434, 30973, 33, 1031, 37, 1031, 14874, 6759, 89, 16317, 198]\n"
     ]
    }
   ],
   "source": [
    "text = textwrap.dedent(\"\"\"\n",
    "Hallo, dies ist ein Text voller toller Wörter\n",
    "\n",
    "Es gibt auch Sonderzeichen - wie super!\n",
    "\n",
    "Bli Bla Blub FoobarBazFazSchmatz......\n",
    "\"\"\")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26247f11-d760-418e-a0b6-601dd949c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.decode(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f73ecad-9066-4cf6-9ea8-e1ac3e2c6ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hallo, dies ist ein Text voller toller Wörter\n",
      "\n",
      "Es gibt auch Sonderzeichen - wie super!\n",
      "\n",
      "Bli Bla Blub FoobarBazFazSchmatz......\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f96c3e2-7825-4ea6-b29a-339e840c6283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 376, 12303, 12303, 12303, 12303, 12303, 12303, 84, 257, 257, 257, 257, 376, 334, 84, 37]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('a Fuuuuuuuuuuuuu a a a a F uuF', allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9ce0c9-7857-4144-a0d6-b278b67159e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64 = a ohne Leerzeichen\n",
    "# 376 = F mit Leerzeichen\n",
    "# 12303 = uu\n",
    "# 84 = u\n",
    "# 257 = a mit Leerzeichen\n",
    "# 376 = F mit Leerzeichen\n",
    "# 334 = u mit Leerzeichen\n",
    "# 84 = u ohne Leerzeichen\n",
    "# 37 = F ohne Leerzeichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e3abfd-1f06-46fc-8c6f-6cad93608e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20479"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as r\n",
    "from pathlib import Path\n",
    "\n",
    "response = r.get(\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "response.raise_for_status()\n",
    "content = response.text\n",
    "Path('the-verdict.txt').write_text(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013e10fe-1bfd-4e5b-adb0-bb170558d095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/ml-playground/gpt/venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff9efd9b-5844-48a1-92ed-45e143010e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Path('the-verdict.txt').read_text()\n",
    "\n",
    "dataset = GPTDatasetV1(text, tokenizer, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f7e906e-34d0-4d0d-932d-affdc24eb957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  40,  367, 2885, 1464]), tensor([ 367, 2885, 1464, 1807]))\n",
      "(tensor([ 367, 2885, 1464, 1807]), tensor([2885, 1464, 1807, 3619]))\n",
      "(tensor([2885, 1464, 1807, 3619]), tensor([1464, 1807, 3619,  402]))\n",
      "(tensor([1464, 1807, 3619,  402]), tensor([1807, 3619,  402,  271]))\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "print(dataset[1])\n",
    "print(dataset[2])\n",
    "print(dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259ec079-98f5-4563-b434-6e019798f268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01fba613-ea18-46d6-a7f7-f32b095fd498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always ---  HAD always thought\n",
      " HAD always thought --- AD always thought Jack\n",
      "AD always thought Jack ---  always thought Jack G\n",
      " always thought Jack G ---  thought Jack Gis\n",
      " thought Jack Gis ---  Jack Gisburn\n",
      " Jack Gisburn ---  Gisburn rather\n",
      " Gisburn rather --- isburn rather a\n",
      "isburn rather a --- burn rather a cheap\n",
      "burn rather a cheap ---  rather a cheap genius\n",
      " rather a cheap genius ---  a cheap genius--\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    inputs, targets = dataset[i]\n",
    "    print(\n",
    "        tokenizer.decode(inputs.tolist()),\n",
    "        \"---\",\n",
    "        tokenizer.decode(targets.tolist())\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894256aa-c590-4f07-89a3-a74c349fe0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "torch.manual_seed(123)\n",
    "vocab_size = tokenizer.max_token_value + 1\n",
    "output_dim = 256 # GPT-3 would have\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fca21b-7217-4bbd-aed9-e35ca0aa2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d612ff-471d-4681-8e3a-2d9091c2fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3eddcbb-6241-4f4d-8910-0521f5ac7b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f05e0974-96e3-4291-9c6d-3105a30d597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 308]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7de2768-e362-459b-8b0b-d7b2025b439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb7905c2-5323-4ba5-90bb-2cb5d8a43ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a190fcb-9067-4799-a069-f3bb4aaf64b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.4150, -0.3142,  0.2827,  ...,  0.8155, -0.1085, -1.1927],\n",
      "        [-1.9800,  0.0610, -0.0494,  ..., -0.6422,  0.5716, -1.1329],\n",
      "        [ 1.0052,  1.7802,  1.2652,  ..., -1.1619, -0.1109,  1.0411],\n",
      "        [ 0.3760, -0.3758, -0.0484,  ...,  0.1080,  0.3852,  1.0876]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a95cfe46-35bf-4465-841e-23eb8170785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4150, -0.3142,  0.2827,  ...,  0.8155, -0.1085, -1.1927],\n",
      "        [-1.9800,  0.0610, -0.0494,  ..., -0.6422,  0.5716, -1.1329],\n",
      "        [ 1.0052,  1.7802,  1.2652,  ..., -1.1619, -0.1109,  1.0411],\n",
      "        [ 0.3760, -0.3758, -0.0484,  ...,  0.1080,  0.3852,  1.0876]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d27134f-4b19-4771-b10e-6d9d78fad1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-Embedding: für jedes Token immer gleich.\n",
    "# Positions-Embedding: für jede Position immer gleich.\n",
    "# Kombination: Token + Position → derselbe Token sieht an anderer Stelle anders aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec748238-040b-4788-82dd-ec3145e58bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb26c9-8249-458e-8b38-8555528bd04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c7917-bb5b-44c2-b34b-8cf7994d752f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4707f4f-bcd3-42a6-ba96-b312037747c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "max_length = 5\n",
    "context_length = max_length\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Token-Embeddings\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# Positions-Embeddings\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b822b559-129b-4b17-ab86-d71fc3ce4c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7e2740e-2f70-4e93-a5b9-3c6cab580113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-Embeddings:\n",
      " tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tokens = torch.tensor([2, 4, 1, 5, 3])\n",
    "token_embeddings = embedding_layer(tokens)\n",
    "print(\"Token-Embeddings:\\n\", token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8d5ef-0c66-4d90-b734-210343a82ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions-Embeddings:\n",
      " tensor([[-0.6307,  1.2340,  0.3127],\n",
      "        [ 0.6972, -0.9950, -1.1476],\n",
      "        [-0.9178,  0.9045, -2.0975],\n",
      "        [ 1.1558, -1.2157,  0.1295],\n",
      "        [ 0.0967,  1.4086,  0.1915]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# das gibt auch die maximale Kontextlänge vor\n",
    "positions = torch.arange(max_length)  # [0, 1, 2, 3, 4]\n",
    "pos_embeddings = pos_embedding_layer(positions)\n",
    "print(\"Positions-Embeddings:\\n\", pos_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c0a7d2d-e3f3-445c-8863-83e053d5e919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kombinierte Input-Embeddings:\n",
      " tensor([[ 6.4463e-01,  1.0331e+00,  1.5211e-01],\n",
      "        [-4.6168e-01, -6.6958e-01, -1.7791e+00],\n",
      "        [-1.0431e-05,  2.4855e+00, -7.9649e-01],\n",
      "        [-1.6842e+00, -2.0005e+00, -1.2800e+00],\n",
      "        [-3.0482e-01,  2.3751e+00, -9.5661e-01]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Kombinierte Input-Embeddings:\\n\", input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415e0a1-089c-4c2e-9495-fd349afd8a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af65c5-e37d-4383-9d06-cc750afd48ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f80afd-290f-487a-a919-aca3c9e18da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec90d2-cd7f-455d-b300-917cd833e0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260794b7-f388-4cce-a5bc-5863a3a2656d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
